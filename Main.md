# THE INCOMPLETENESS OF OBSERVATION
### Why Quantum Mechanics and General Relativity Cannot Be Unified From Within

**Author:** Alex Maybaum  
**Date:** February 2026  
**Status:** DRAFT PRE-PRINT  
**Classification:** Theoretical Physics / Foundations

---

## ABSTRACT

The incompatibility between quantum mechanics and general relativity is widely viewed as a failing of modern physics that requires a novel unifying framework. This paper proposes the opposite: the incompatibility is a structural consequence of embedded observation. Any observer that is part of the continuous universe it measures must access reality through projections that discard inaccessible degrees of freedom defined by spacetime's causal boundaries. 

By applying Wolpert’s (2008) physical limits of inference, we show that quantum and gravitational vacuum measurements are complementary projections of a shared, causally disconnected hidden sector. This reframes the $10^{122}$ cosmological constant discrepancy as a direct measurement of roughly $10^{244}$ hidden-sector degrees of freedom.

Furthermore, mathematically tracing out this immense trans-horizon sector via the Nakajima-Zwanzig formalism generates a non-local memory kernel. We show that the cosmological horizon trace-out is provably non-Markovian—the timescale separation required for memoryless dynamics collapses when bath, system, and relaxation timescales all equal the Hubble time—and argue that conservation-law-enforced correlations and non-perturbative gravitational coupling further force CP-indivisible dynamics. Existing counterexamples (Kaplanek-Burgess Markovian evolution near horizons) are confined to weak-coupling regimes where the timescale hierarchy holds and do not apply to the full cosmological trace-out. By Barandes' stochastic-quantum correspondence, the resulting indivisible dynamics recover the Schrödinger equation as the mandatory description of an embedded observer's marginal predictions. The framework yields falsifiable predictions, including a 54 ms gravitational wave echo for a $30 M_\odot$ black hole, and identifies temporal Tsirelson's bound as a sharp falsification criterion for the trace-out dynamics. Finally, by defining the wave function strictly as an epistemic data compression algorithm, this framework natively resolves the Wigner's Friend paradox, dissolves the Everettian measure problem, reinterprets black hole singularities and the information paradox as consequences of observational incompleteness, and explains why the AdS/CFT correspondence and string theory succeed mathematically while remaining structurally inapplicable to embedded de Sitter observers. Because the hidden-sector dimensionality is tied to the evolving cosmological horizon area, the framework predicts a slowly time-dependent effective cosmological constant in the Running Vacuum Model form, $\Lambda_{\text{eff}} = \Lambda_0 + 3\nu_{\text{OI}}(H^2 - H_0^2)/(8\pi G)$, derived from information-theoretic rather than renormalization-group arguments and testable by DESI, Euclid, and the Vera Rubin Observatory.

---

## I. INTRODUCTION

### 1.1 The Incompatibility Problem
Quantum mechanics and general relativity are extraordinarily successful yet structurally incompatible. Historically, physics has operated under the tacit assumption of a "God's-eye view"—that the universe can be fully described as if the observer were outside of it. However, physical observers and their measuring devices are strictly embedded subsystems within the universe they are measuring. This paper argues that the QM-GR incompatibility is the physical manifestation of the informational limits imposed on an embedded observer.

### 1.2 The Limits of Embedded Inference
Wolpert (2008) proved mathematically that any physical inference device faces absolute limits on what it can know about the universe it inhabits. Because an embedded device is smaller than the total system, its observations must be surjective, many-to-one mappings that discard information. 

Specifically, Wolpert established the "Blind Spot" Theorem, which proves that there is always at least one fact about the universe that an embedded observer fundamentally cannot determine, and the "Mutual Inference" Impossibility, which demonstrates that two inference devices using different projections cannot fully reconstruct each other's conclusions. Crucially, an inference device does not need to be a conscious scientist or a computer. Spacetime itself acts as a physical inference device that couples to the vacuum, just as localized matter acts as an inference device coupling to local fluctuations. Because both are embedded subsystems, they are bound by the same exact mathematical limits.

We can partition the total state space of the universe into two sectors: degrees of freedom accessible to an observer (the visible sector) and degrees of freedom that are strictly inaccessible (the hidden sector). The hidden sector consists of standard physics rendered inaccessible by spacetime's causal structure: primarily trans-horizon modes beyond the observable universe and the interiors of black holes.

### 1.3 Formal Mapping to Wolpert's Framework

To rigorously apply Wolpert's limits, we must show that the two physical couplings—localized matter to the vacuum, and spacetime geometry to the vacuum—satisfy the formal requirements of his inference-device framework. Define the total state space $\Omega$ as the full phase space of all field configurations across the complete Cauchy surface, including both the observable patch and the trans-horizon sector.

**Device 1 — Localized Matter.** Any localized matter system (e.g., an atom subject to the Lamb shift) constitutes an inference device with observation map

$$\pi_Q : \Omega \to \mathbb{R}, \qquad \pi_Q(\omega) = \langle \hat{\phi}^2 \rangle_\omega$$

where $\omega \in \Omega$ is the full-universe configuration and the output is the local expectation value of the squared field at the device's location. This map is: (i) *Embedded*—the matter system is a proper subsystem of $\Omega$, occupying a finite worldtube; and (ii) *Surjective (many-to-one)*—distinct full configurations $\omega \neq \omega'$ that differ only in trans-horizon modes yield $\pi_Q(\omega) = \pi_Q(\omega')$. The map's kernel is precisely the hidden sector.

**Device 2 — Spacetime Geometry.** The local gravitational field constitutes a second inference device with observation map

$$\pi_G : \Omega \to \mathbb{R}, \qquad \pi_G(\omega) = \langle T_{00} \rangle_\omega$$

where the output is the net signed stress-energy density sourcing Einstein's equations at the device's location. This map is: (i) *Embedded*—the metric is itself a dynamical degree of freedom within $\Omega$; and (ii) *Surjective*—distinct configurations with different individual mode contributions $s_i|E_i|$ can produce the same net $T_{00}$. The kernel is again the hidden sector, but the map integrates over it differently.

**Verification of Wolpert's Conditions.** Both devices satisfy the three requirements of Wolpert's formalism: (i) each is a proper physical subsystem of the universe, (ii) each observation map is a many-to-one projection from $\Omega$ to a lower-dimensional output, and (iii) the two maps have non-identical kernels—$\pi_Q$ discards sign information while summing magnitudes, whereas $\pi_G$ retains signs while summing algebraically. The relevant criterion is not intentionality but structure: any physical subsystem whose state is a deterministic, many-to-one function of the total configuration satisfies Wolpert's formal requirements regardless of whether the mapping constitutes "computation" in the colloquial sense. Wolpert himself emphasizes that his framework is substrate-independent [4].

**Application.** By Wolpert's Mutual Inference Impossibility, two embedded devices with non-identical surjective projections cannot fully reconstruct each other's conclusions from their own outputs alone. Therefore, no single embedded subsystem can simultaneously access both the unsigned total ($\pi_Q$) and the signed net ($\pi_G$) of the vacuum—the discrepancy between them is a structural *limit*, not an error awaiting correction.

---

## II. OBSERVATIONAL INCOMPLETENESS

### 2.1 Complementary Projections

The sharpest conflict between QM and GR is the cosmological constant problem [1, 2, 3]: quantum mechanics predicts a vacuum fluctuation energy of roughly $10^{113} \text{ J/m}^3$, whereas general relativity measures a net gravitational effect of only about $6 \times 10^{-10} \text{ J/m}^3$. This creates a discrepancy ratio of $10^{122}$. Rather than assuming this is a fine-tuning error, the embedded observation framework interprets this as two fundamentally different physical inference devices making complementary projections of the same hidden sector. 

This can be conceptualized through a classic thermodynamic analogy. When measuring a glass of water, a thermometer records the total unsigned kinetic energy of all molecules (a variance-type measurement), producing a massive reading because nothing cancels out. Conversely, a suspended dust speck is only moved by the net canceled-out mechanical push of molecules striking it from all sides (a mean-type measurement), leaving a tiny statistical residual. 

* **The Variance Target (Localized Matter):** Localized matter measuring the vacuum (e.g., via the Lamb shift) acts as the thermometer. Because the field operator is squared ($\hat{\phi}^2$), its expectation value is strictly positive-definite for all modes. This measures the unsigned total activity: $V \propto \sum_{i=1}^{N} \langle \hat{\phi}_i^2 \rangle = N \langle E \rangle$.
* **The Mean Target (Spacetime):** The local gravitational field acts as the dust speck. It couples dynamically to the net signed sum of the stress-energy tensor. This is a mean-type projection: $M = \langle T_{00} \rangle = \sum_{i=1}^{N} s_i |E_i|$, where $s_i \in \{+1, -1\}$ is the spin-statistics sign of the $i$-th mode.

Near a causal horizon, the Unruh temperature [18] $T_U = \frac{\hbar a}{2\pi k_B c}$ diverges as proper acceleration $a \to \infty$. In this ultra-relativistic, infinite-temperature limit ($T_U \gg m_i$), the active trans-horizon modes behave as a randomized, maximally mixed conformal fluid. Consequently, the spin-statistics sign $s_i$ over the $N$ modes functions as an independent, identically distributed random variable with a mean $\langle s_i \rangle = 0$.

By the Central Limit Theorem, the expected value of the gravitational sum is zero, but its root-mean-square fluctuation is strictly non-zero and scales with the square root of the number of states:
$$M_{\text{rms}} = \sqrt{\sum_{i=1}^{N} (s_i |E_i|)^2} = \sqrt{N} \langle |E_i| \rangle$$

Therefore, the ratio of the variance projection to the mean-squared projection strictly scales as the square root of the hidden-sector dimensionality:
$$\frac{V}{M_{\text{rms}}} \approx \frac{N \langle |E_i| \rangle}{\sqrt{N} \langle |E_i| \rangle} = \sqrt{N}$$

> **Observational Incompleteness Principle.** Let the universe be partitioned into visible and hidden sectors, and let the observer's projection from the full state to the visible sector be many-to-one. Define the variance-type target $V = \sum |E_i|$ and the mean-type target $M = \sum E_i$. No single embedded inference device can simultaneously determine both targets with joint accuracy exceeding Wolpert's bounds. The continuous precision corollary forces a nontrivial product bound on their mean-squared errors.

**Clarification: independence of spin-statistics signs under trans-horizon entanglement.** The TFD entanglement structure invoked in §3.2 entangles *amplitude* (occupation number) degrees of freedom between matched visible and trans-horizon mode pairs. The spin-statistics sign $s_i$, by contrast, is not a dynamical variable but a fixed topological property of each field species: $s_i = +1$ for bosonic and $s_i = -1$ for fermionic contributions at each energy scale. In the ultra-relativistic regime ($T_U \gg m_i$), all Standard Model species contribute at comparable magnitudes, so the net sign at each energy bin is determined by species composition with no systematic bias. The CLT therefore applies to the *species-sign sector*, while entanglement resides in the *amplitude sector*; the two structures coexist without contradiction.

The iid assumption on the signs $s_i$ can be relaxed. Even under residual correlations between nearby energy scales, the $\sqrt{N}$ scaling is preserved provided the correlations decay sufficiently fast. Specifically, for a fast-scrambling horizon bath (§3.2), correlations decay exponentially on the scrambling timescale $t_s \sim \ln S$, satisfying the strong mixing ($\alpha$-mixing) condition: $\alpha(k) \to 0$ as the separation $k$ between energy bins grows. Bradley’s central limit theorem for $\alpha$-mixing sequences [40] guarantees that $M/\sqrt{N} \to \mathcal{N}(0, \sigma^2)$ with $\sigma^2 = \text{Var}(s_i) + 2\sum_{k=1}^{\infty} \text{Cov}(s_0, s_k)$, where the covariance sum converges due to exponential decay. The $\sqrt{N}$ scaling is therefore robust to short-range correlations; only long-range power-law correlations with exponent $\alpha < 1/2$ could alter the scaling to $N^\alpha$, which would change the predicted discrepancy ratio from $10^{122}$ to $10^{244\alpha}$. Since the observed discrepancy provides a single data point, such a modification would constrain $\alpha$ rather than falsify the framework—but would require a physical mechanism for long-range sign correlations across widely separated energy scales, which the scrambling dynamics disfavor.

### 2.2 Deriving the $10^{122}$ Discrepancy
The ratio of the two projections directly encodes the hidden sector's dimensionality. Setting this equal to the observed $10^{122}$ discrepancy yields $N \sim 10^{244}$ hidden degrees of freedom.

This specific number corroborates holographic principles [6]. The Bekenstein-Hawking entropy of the cosmological horizon [10] is $S_{\text{dS}} \sim 10^{122}$. Because every visible degree of freedom can correlate with every hidden degree of freedom, the total number of correlations is the product of their state spaces ($S_{\text{visible}} \times S_{\text{hidden}}$). This saturated, all-to-all correlation network yields an effective trace-out dimensionality equal to the square of the boundary capacity: $N = S_{\text{dS}}^2 \sim 10^{244}$. The cosmological constant problem is therefore not an error; it is the most precise measurement of the dimensionality of the inaccessible universe.

---

## III. THE EMERGENCE OF QUANTUM MECHANICS

### 3.1 Classical Axioms and the Trace-Out Operation
Having established the macroscopic scale of the hidden sector, we now ask how an embedded, localized particle behaves when subjected to this immense informational deficit. This framework derives quantum mechanics purely from three classical premises:
1.  **Classical Continuous Dynamics:** The total universe evolves deterministically via the continuous Liouville equation: $\frac{\partial \rho}{\partial t} = \{H, \rho\}$.
2.  **Classical General Relativity:** Einstein's field equations define the absolute information barriers of the hidden sector. 
3.  **Classical Probability Theory:** Observational predictions are classical expectation values.

Because the universe is fundamentally continuous and deterministic at the global level, an embedded observer's inability to track the $10^{244}$ hidden states forces a massive mathematical data compression. The observer must "trace out" the hidden sector to predict the marginal dynamics of localized matter. Section 3.2 demonstrates that this trace-out is provably non-Markovian for IR horizon modes and gives strong physical arguments that the dynamics are in fact CP-indivisible—the key condition required for quantum mechanics to emerge from Barandes' stochastic-quantum correspondence.

### 3.2 From Trace-Out to Quantum Dynamics

Tracing out the hidden sector via the Nakajima-Zwanzig formalism [12, 13, 20] yields a Generalized Langevin Equation with a non-local memory kernel $\gamma(t-\tau)$, where the $10^{244}$ trans-horizon states act as a continuous background noise $\xi(t)$:
$$m\ddot{x}(t) = -\nabla V(x) - \int_{0}^{t} \gamma(t-\tau)\dot{x}(\tau)d\tau + \xi(t)$$

A standard objection is that tracing out a vast bath should immediately lead to classical decoherence—incoherent, memoryless noise that destroys quantum behavior. For a generic thermal bath, this is correct: the Markovian (memoryless) approximation holds whenever the bath correlation time is much shorter than the system's dynamical timescale, and the resulting dynamics are CP-divisible—each time step is an independent, information-destroying channel. We now show that the cosmological horizon trace-out fails every prerequisite for this conclusion, through three independent and progressively stronger arguments.

**Argument 1: Timescale collapse.** The derivation of any Markovian master equation—Lindblad, Redfield, or Davies—requires a strict separation of three timescales [20]: the bath correlation time $\tau_B$ must be much shorter than the system's characteristic evolution timescale $\tau_S$, which must itself be much shorter than the relaxation time $\tau_R$:

$$\tau_B \ll \tau_S \ll \tau_R$$

For a localized qubit with transition frequency $\omega$ coupled to a thermal bath, this hierarchy is readily satisfied: $\tau_B \sim 1/T$, $\tau_S \sim 1/\omega$, $\tau_R \sim 1/(\lambda^2 \omega)$, and the ratios are controlled by the small coupling $\lambda$. For the cosmological horizon, all three timescales collapse to the same order:

- $\tau_B \sim 1/H$ — the bath correlation time for IR horizon modes, set by the Gibbons-Hawking temperature $T_{\text{GH}} = H/2\pi$
- $\tau_S \sim 1/H$ — the dynamical timescale of cosmological observables (the Hubble time)
- $\tau_R \sim 1/H$ — the de Sitter equilibration time

When $\tau_B \approx \tau_S \approx \tau_R$, the Markovian approximation fails by its own defining criterion. This is not a subtle argument about the structure of the bath state—it is a direct, quantitative failure of the most basic prerequisite. The dynamics of the visible sector at cosmological scales are provably non-Markovian for the IR modes that dominate the vacuum energy.

**Argument 2: Conservation-law-enforced correlations.** CP-divisible dynamics satisfies a monotonicity condition: the trace distance $D(\rho_1(t), \rho_2(t))$ between any two system states can only decrease or remain constant over time. Physically, the system can only lose distinguishability to the environment, never recover it—information flows out but not back. However, exact conservation of stress-energy across the cosmological horizon forces specific correlations between the visible and hidden sectors to persist indefinitely. If two visible-sector configurations differ in total energy by $\Delta E$, the corresponding hidden-sector configurations must differ by $-\Delta E$. This correlation is enforced by the constraint equations of general relativity and cannot decay under any dynamics that respects diffeomorphism invariance. In a weakly coupled system, the conserved-quantity correlations are perturbatively small ($\sim \lambda^2$) and compatible with approximately CP-divisible dynamics. But for the cosmological trace-out, the energy correlation between visible and hidden sectors is the dominant feature—it is what produces the $10^{122}$ discrepancy. The persistent mutual information between sectors is not a small correction; it is the largest term in the problem. These conservation-law-enforced correlations periodically drive information backflow from hidden to visible sector as modes cross and recross the slowly evolving horizon, violating the monotonicity that CP-divisibility requires (Babu et al. [21]).

**Argument 3: No perturbative route to CP-divisibility.** Every existing proof that dynamics near a horizon can be rendered Markovian—the Davies-Lindblad weak-coupling limit, Merkli's error bounds [41], the Kaplanek-Burgess open EFT resummation [37, 38]—explicitly requires that the system-bath coupling constant $\lambda$ be small, with errors of order $O(|\lambda|^{1/4})$ or $O(g^2)$. The gravitational coupling of the entire visible sector to the trans-horizon sector is not perturbatively small. It is what produces the $10^{122}$ projection discrepancy of §2.2. No perturbative Markovian result applies at this coupling strength. This does not by itself prove CP-indivisibility, but it proves that no known mathematical route to CP-divisibility is available for the cosmological trace-out.

**Additional structural constraints.** Three further properties of the trans-horizon bath reinforce and sharpen the above arguments:

- *Maximal entanglement.* The vacuum state across a causal horizon takes the Thermofield Double (TFD) form. As established by Buscemi [23], initially entangled system-environment states violate the product-state assumption required for complete positivity. For almost any entangled initial state, the set of joint unitaries generating CP-divisible reduced dynamics is measure-zero in the space of all unitaries.
- *Spectral rigidity from fast scrambling.* Causal horizons scramble information exponentially fast (Sekino and Susskind [19]). The resulting spectral rigidity—manifesting as a non-decaying ramp in the Spectral Form Factor (Cotler et al. [22])—prevents bath correlation functions from decaying to zero. The non-decaying component is exponentially small in the entropy ($\sim e^{-S}$), but persists to the Heisenberg time $t_H \sim e^S$, ensuring that the memory kernel never fully decays.
- *Born approximation failure.* Macroscopic conservation laws physically prevent the environment from resetting to equilibrium after each interaction, forcing persistent system-environment correlations ($\chi(t) \neq 0$) that invalidate the Born approximation independently of coupling strength (Babu et al. [21]).

**Counterexamples and their scope.** An important body of work by Kaplanek and Burgess [37, 38] demonstrates that an accelerated Unruh-DeWitt detector can evolve via a Markovian Lindblad equation at late times near a Rindler or Schwarzschild horizon. This is fully consistent with the above arguments: their results apply to a localized qubit whose transition frequency $\omega$ satisfies $\omega \gg H$, precisely the regime where the timescale separation of Argument 1 holds ($\tau_S \sim 1/\omega \ll \tau_B \sim 1/H$), and where the coupling is perturbatively weak (Argument 3). Kaplanek and Burgess themselves identified that as $\omega \to H$ (or equivalently $m/H \to 0$), the Markovian validity window closes due to "critical slowing down" [39]—the thermalization timescale diverges and the Markovian resummation breaks down. Their counterexamples therefore confirm rather than contradict the present analysis: Markovian dynamics emerge when the timescale hierarchy is satisfied, and fail when it collapses.

We state the central dynamical claim:

> **The Trace-Out Conjecture.** Tracing out the trans-horizon sector in the cosmological setting forces the marginal dynamics of the visible sector into a CP-indivisible stochastic process. Any faithful mathematical representation of these dynamics is empirically equivalent to quantum mechanics.

The status of this claim is stronger than a bare conjecture but short of a rigorous proof. What is *established* is: (a) the dynamics are provably non-Markovian for IR horizon modes, by the timescale collapse argument; (b) conservation laws force persistent system-environment correlations that are non-perturbatively large, driving information backflow incompatible with CP-divisible monotonicity; (c) no known perturbative route to CP-divisibility exists at the relevant coupling strength; and (d) every additional structural property of the trans-horizon bath (TFD entanglement, spectral rigidity, Born approximation failure) independently makes CP-divisibility more fragile. What remains *open* is a fully non-perturbative mathematical proof that these conditions jointly force at least one canonical decay rate $\gamma_k(t) < 0$ at some time in the cosmological regime. We note that the framework's falsifiable predictions (§4.7, §4.8) do not depend on the specific mechanism producing indivisibility, only on the fact of it; if the dynamics are CP-indivisible for any reason, Barandes' correspondence [14, 15] guarantees the framework follows.

By Barandes' stochastic-quantum correspondence [14, 15], any indivisible stochastic process on configuration space is mathematically equivalent to a unitarily evolving quantum system. The Schrödinger equation is therefore the unique description an embedded observer must use to predict the marginal behavior of a system coupled to this bath. The indivisible stochastic process recovers the quantum potential $Q = -\frac{\hbar^2}{2m} \frac{\nabla^2 \sqrt{\rho}}{\sqrt{\rho}}$ as the effective force on configuration-space probability flow, with diffusion coefficient $D = \hbar/2m$. The value of Planck's constant is consistent with the ratio of the holographic entropy to the hidden-sector dimensionality, $\hbar \approx S_{\text{universe}}/N \approx 10^{-122}$ in dimensionless units—a non-trivial self-consistency check linking the macroscopic causal structure to the microscopic noise floor.

Le et al. [17] proved that CP-divisible dynamics satisfies temporal Tsirelson's bound ($B \le 2\sqrt{2}$). This provides a sharp falsification criterion: if the trace-out of this constrained hidden sector violates temporal Tsirelson's bound, it is provably CP-indivisible and inherently quantum. Conversely, if the cosmological dynamics can be shown to *satisfy* the bound, the conjecture would be significantly weakened.

---

## IV. IMPLICATIONS & PREDICTIONS

### 4.1 The Double-Slit Experiment as Stochastic Guidance
The double-slit experiment is natively explained through continuous, single-universe fluid dynamics. The particle does not become a delocalized wave; it remains a localized particle that travels through a single slit. However, opening or closing a second slit alters the macroscopic boundary conditions of the environment. This instantly modifies the flow of the continuous, fluid-like trans-horizon background noise, reshaping the configuration-space pressure gradient (the Quantum Potential) that physically guides the particle's stochastic trajectory toward the interference fringes.

This picture shares structural features with de Broglie–Bohm pilot-wave theory, which also posits a localized particle guided by a quantum potential. The key difference is ontological: in Bohmian mechanics the guiding wave is a fundamental, ontic entity, whereas here the quantum potential is an emergent consequence of tracing out the trans-horizon bath. The framework therefore does not inherit Bohmian mechanics’ requirement for a preferred foliation of spacetime, since the stochastic dynamics are defined on the configuration space of the visible sector rather than on a global pilot wave.

### 4.2 Bell’s Theorem and Dynamical Indivisibility
A standard objection to any stochastic underpinning of quantum mechanics is Bell’s Theorem [11], which proves that no theory satisfying certain assumptions can reproduce quantum correlations. The precise assumption is *factorizability*: conditioning on the complete hidden variable $\lambda$ in the common past of two measurements makes their outcomes statistically independent,
$$P(a,b|x,y,\lambda) = P(a|x,\lambda) \cdot P(b|y,\lambda)$$
There are three logically distinct ways to violate this condition: (i) *nonlocality*—a direct causal influence between the distant measurements, (ii) *superdeterminism*—the measurement settings $x, y$ are themselves correlated with $\lambda$, removing the experimenters’ freedom of choice, and (iii) *dynamical indivisibility*—the stochastic evolution connecting preparation to outcomes is irreducibly global and cannot be decomposed into independent local substeps.

This framework takes the third route. As shown in §3.2, the trans-horizon bath forces non-Markovian, and on strong physical grounds CP-indivisible, dynamics. In Barandes’ formalization [14, 15, 16], the transition maps of an indivisible stochastic process from preparation time $t_0$ to measurement time $t_2$ cannot be factored through any intermediate time $t_1$: the global map is not the composition of two local maps. Consequently, the joint probability $P(a,b|x,y,\lambda)$ inherits this irreducible global structure and does not factorize—even though each measurement is performed locally and the experimenters retain full freedom in choosing their settings. Measurement independence is preserved; what fails is the assumption that the dynamical law connecting the shared past to the two distant outcomes can be decomposed into independent, memoryless local processes. No faster-than-light signaling is required. This resolution of Bell's theorem depends on the dynamics being CP-indivisible; the timescale collapse and conservation-law arguments of §3.2 provide strong grounds for this, but a fully non-perturbative proof remains open.

### 4.3 Wigner's Friend and Relational Epistemology

The Wigner's Friend paradox exposes the conceptual friction between objective physical events and subjective quantum states. By defining the wave function entirely as an algorithmic data compression tool, this framework natively resolves the paradox. Inside the isolated laboratory, the Friend performs a measurement, and a definite, objective physical event occurs—driven by the classical divergence of stochastic trajectories in the local background noise. Stationed outside, Wigner lacks access to that local microscopic information. Because he cannot track the exact stochastic trajectory, Wigner is mathematically forced to apply the trace-out algorithm and assign a superposition to the lab. This superposition is strictly a measure of Wigner's own epistemic deficit, not the physical state of the Friend.

### 4.4 The Everettian Illusion and the Measure Problem

The Many-Worlds Interpretation (MWI) posits that the universal wave function is the fundamental, ontic physical reality, resolving quantum superpositions by continuously splitting the universe into deterministic branches. If the present framework is correct, MWI commits a structural category error: it mistakes a mandatory epistemic data compression algorithm for the physical universe itself. 

Formulating a perfectly deterministic universal wave function implicitly requires an observer entirely outside the system. By applying Wolpert's limits, the universal wave function is an informational impossibility for any embedded physical observer. In this framework, the total universe evolves deterministically as a single, continuous reality governed by the classical Liouville equation. What MWI interprets as newly created, parallel universes actually corresponds to the unmeasured, fluctuating configurations of the hidden sector within our single universe. The realization of an outcome is not the universe splitting, but simply the classical divergence of stochastic trajectories driven by the indivisible macroscopic background noise.

Furthermore, MWI struggles to organically derive the Born rule ($p = |\psi|^2$) in a universe where all outcomes deterministically occur. In the embedded framework, this problem dissolves: because there is only a single continuous universe, probabilities are standard, classical stochastic probabilities arising directly from epistemic ignorance. Given the CP-indivisibility argued in §3.2, the Born rule is natively recovered as the equilibrium distribution of the indivisible stochastic process generated by the trans-horizon bath.

### 4.5 Black Hole Singularities and the Information Paradox

The interior of a black hole is the paradigmatic example of the hidden sector: a region rendered strictly inaccessible by a causal horizon. In classical general relativity, extrapolating the field equations through this horizon predicts a singularity—a point of infinite curvature where the theory breaks down. The embedded observation framework reinterprets this breakdown. A singularity is what results from extending a visible-sector description past its domain of validity, into a region that belongs entirely to the hidden sector. It is a mathematical artifact of the God’s-eye-view assumption, not a physical event. Just as the $10^{122}$ cosmological constant discrepancy is not a fine-tuning error but a measurement of hidden degrees of freedom (§2.2), the singularity is not a prediction of nature but a signal that the observer’s projection map $\pi_G$ has been applied outside its domain.

This reframing is consistent with a near-universal consensus across quantum gravity programs that singularities must be resolved in any complete theory. Loop quantum gravity replaces the singularity with a quantum bounce at Planck density [24]. String theory regularizes it through non-perturbative corrections [25]. Recent work in higher-derivative gravity shows that singularity resolution can emerge from pure gravity alone, without additional matter fields [26]. The embedded observation framework provides a unifying explanation for *why* every approach resolves the singularity: all are, in different formalisms, recognizing that the classical description fails at the boundary of the hidden sector.

The black hole information paradox dissolves under the same logic. Hawking’s original argument [27] assumes that information falling past the horizon is destroyed at the singularity, leading to non-unitary evolution. But if the singularity is not physical and the interior is simply the hidden sector, then information is not destroyed—it is *inaccessible*. The exterior observer must trace out the interior degrees of freedom, producing exactly the thermal Hawking spectrum as their marginal prediction. The apparent information loss is an instance of observational incompleteness, not a violation of unitarity. The Page curve—the expected rise and fall of radiation entropy during evaporation—has recently been shown to emerge generically in open quantum systems at weak coupling [28], supporting the interpretation that it reflects the observer’s restricted access rather than a uniquely gravitational phenomenon.

### 4.6 String Theory, AdS/CFT, and the Limits of External Unification
This framework offers a structural explanation for the pattern of successes and difficulties in string theory. The AdS/CFT correspondence (Maldacena [7]) achieves an exact, mathematically rigorous unification of quantum mechanics and gravity by placing the observer on the *asymptotic boundary* of a hypothetical Anti-de Sitter space, looking inward. Because this observer is positioned on the outside looking in, they inherently possess a "God's-eye view" and are entirely immune to Wolpert's absolute limits for embedded observers. The unification is exact precisely because it is constructed from an external vantage point.

However, our actual universe is expanding (de Sitter space) and has no outer boundary. We are permanently embedded inside it, rendering the AdS/CFT exact unification structurally inapplicable to us. The embedded observation framework predicts that this is not a technical obstacle awaiting a clever de Sitter extension, but a fundamental limit: any exact unification requires access to the complete state space $\Omega$, which Wolpert's theorem prohibits for embedded observers.

This perspective also addresses string theory's landscape problem. String theory admits an enormous number of self-consistent vacuum solutions — estimated at $10^{500}$ or more — with no known mechanism for selecting our universe from among them. The embedded observation framework suggests that this is not a failure of the theory but a consequence of attempting to construct a unique, complete description from within. A God's-eye observer with access to the full hidden sector could in principle identify the correct vacuum; an embedded observer, restricted to the projections $\pi_Q$ and $\pi_G$, cannot — and the resulting degeneracy is the vacuum-selection analogue of the $10^{122}$ cosmological constant discrepancy. More broadly, string theory's extraordinary mathematical consistency — and its persistent inability to make unique, falsifiable predictions for our universe — is consistent with the framework's central thesis: exact unification is achievable only from a vantage point that embedded observers cannot occupy.

### 4.7 Falsifiable Predictions
Because the quantum-gravity discrepancy is mapped to structural boundary limits, the global cosmological incompleteness theorem must scale down to local event horizons. 

* **Gravitational Wave Echoes:** The classical event horizon is replaced by an informational boundary located a microscopic distance $\epsilon \approx l_p$ outside the Schwarzschild radius $r_h$. Calculating the tortoise coordinate integral predicts a precise time delay for gravitational wave echoes:
$$\Delta t_{\text{echo}} \approx \frac{r_h}{c} \ln\left(\frac{r_h}{\epsilon}\right)$$
For a stellar-mass black hole remnant of $M = 30 M_\odot$, the expected delay is approximately 54 ms. The identification $\epsilon \approx l_p$ is physically motivated by the Planck-scale breakdown of semiclassical geometry but is not derived from the framework’s internal logic; a different value of $\epsilon$ would shift the predicted echo time logarithmically. To date, LIGO–Virgo–KAGRA searches for post-merger echoes have yielded null results [8], with the original claims of Abedi et al. remaining contested. A continued null result at the predicted timescale would constrain the location of the informational boundary but would not falsify the broader framework, since $\epsilon$ is a free parameter; what *would* falsify it is the complete absence of any echo structure at all scales. Crucially, a *population* of echo detections across mergers with different remnant masses would overconstrain $\epsilon$. The predicted timescale $\Delta t_{\text{echo}} \propto (GM/c^3)\ln(GM/c^2\epsilon)$ depends on mass logarithmically but on $\epsilon$ through the same logarithm; echo detections at consistent timescales across, say, 10, 30, and 60 $M_\odot$ remnants would either confirm or rule out a Planck-scale $\epsilon$ with no remaining freedom. A single detection has a free parameter; three or more do not.
* **Stochastic Gravitational Noise Floor:** Hidden-sector fluctuations must source a continuous stochastic background with an inverse-frequency-squared spectrum in the MHz–GHz band. Next-generation high-frequency gravitational wave detectors [9] could provide an independent test of this prediction.

### 4.8 Cosmological Evolution of Observational Incompleteness

The hidden-sector dimensionality $N$ derived in §2.2 is not a fixed constant. It is set by the Bekenstein-Hawking entropy of the cosmological horizon, $S_{\text{dS}} \sim A/4l_p^2$, which depends on the horizon area and therefore evolves with the expansion history of the universe. In an accelerating universe, this evolution has a specific and potentially observable consequence.

The cosmological event horizon — the boundary beyond which events occurring now will never be observable — has a finite comoving radius that is *shrinking*. As the expansion accelerates, objects currently inside the observable universe are being pushed beyond this horizon permanently. Each departure adds degrees of freedom to the hidden sector and removes them from the visible sector. The particle horizon (the boundary of what light has had time to reach us since the Big Bang) continues to grow, so the volume of *past* events we can observe slowly increases. But the event horizon — the boundary of what we will *ever* observe — is contracting in comoving terms. In the far future, as the universe asymptotes to pure de Sitter space, essentially everything outside the gravitationally bound local group will have crossed beyond the horizon.

Because $N = S_{\text{dS}}^2$ and the horizon entropy evolves with the Hubble parameter as $S_{\text{dS}} \sim 1/H^2$, the hidden-sector dimensionality inherits a specific time dependence. The ratio of the two projections (§2.1) becomes epoch-dependent:

$$\frac{V}{M_{\text{rms}}} = \sqrt{N(t)} = S_{\text{dS}}(t) \sim \frac{1}{H(t)^2}$$

An important subtlety must be addressed before extracting the observational prediction. A naive reading of this scaling might suggest $\Lambda_{\text{eff}}(t) \propto H(t)^2$ as a standalone dark energy model. However, as Hsu [30] demonstrated, a pure $\rho_\Lambda \propto H^2$ scaling in a spatially flat FRW universe yields an effective equation of state $w = 0$: the dark energy mimics pressureless dust and cannot drive cosmic acceleration. This is because if $\rho_\Lambda = \alpha H^2$, the Friedmann equation $H^2 \propto \rho_m + \rho_\Lambda$ forces $\rho_\Lambda / \rho_{\text{total}}$ to remain constant throughout cosmic history, precluding a deceleration-to-acceleration transition.

The resolution lies in distinguishing the *present-day value* of the gravitational projection from its *evolution*. The observed cosmological constant $\Lambda_0$ is the current epoch's gravitational residual $M_{\text{rms}}(t_0)$, set by today's hidden-sector dimensionality $N(t_0)$. This value is an empirical fact anchored to the present horizon area. What evolves is the *deviation* from this value as the horizon changes. Expanding the projection ratio around the present epoch:

$$\Lambda_{\text{eff}}(t) = \Lambda_0 + \frac{d\Lambda_{\text{eff}}}{dH^2}\bigg|_{H_0} (H^2 - H_0^2) + \mathcal{O}((H^2 - H_0^2)^2)$$

The derivative is calculable from the $\sqrt{N}$ scaling. Since $M_{\text{rms}} \propto 1/\sqrt{N} \propto H^2$ and $\Lambda_0 \gg \delta\Lambda$, the leading correction takes the form:

$$\Lambda_{\text{eff}}(H) = \Lambda_0 + \frac{3\nu_{\text{OI}}}{8\pi G}(H^2 - H_0^2)$$

where $\nu_{\text{OI}}$ is a dimensionless coefficient encoding the rate at which the hidden-sector dimensionality responds to changes in the Hubble parameter. This is precisely the form of the Running Vacuum Model (RVM) [31, 32], arrived at here from information-theoretic rather than renormalization-group arguments. The constant offset $\Lambda_0$ is not imposed by hand but emerges naturally as the present-epoch gravitational residual; the $H^2$-dependent correction captures the slow migration of degrees of freedom across the cosmological horizon.

**Relationship to the standard RVM and coefficient comparison.** In the standard RVM, the coefficient $\nu$ arises from QFT renormalization group running of the vacuum energy and is predicted to scale as $\nu_{\text{RVM}} \sim \frac{1}{12\pi} \sum_i M_i^2/M_{\text{Pl}}^2$ [31], giving $\nu_{\text{RVM}} \sim 10^{-3}$ for GUT-scale particles. In the present framework, $\nu_{\text{OI}}$ encodes the logarithmic derivative of the hidden-sector dimensionality with respect to the Hubble parameter. A direct estimate from $N = S_{\text{dS}}^2 \sim H^{-4}$ gives $\nu_{\text{OI}} \sim \Lambda_0 / (M_{\text{Pl}}^2 H_0^2) \sim \Omega_\Lambda \sim 0.7$, which is far too large — it would predict order-unity deviations from $\Lambda$CDM, which are excluded. This discrepancy indicates that the linear expansion above is oversimplified: the full relationship between $\Lambda_{\text{eff}}$ and $N$ must include a suppression mechanism.

There are two natural candidates for this suppression within the framework, and we can estimate their effects to bound the problem from both sides.

*Upper bound (naive estimate).* The naive $\nu_{\text{OI}} \sim \Omega_\Lambda \sim 0.7$ derived above treats every change in $H$ as producing a proportional change in $\Lambda_{\text{eff}}$. This is excluded by observation (DESI + Planck constrain $\nu < 10^{-2}$), establishing that the full mapping from $N$ to $\Lambda_{\text{eff}}$ must include suppression.

*Lower bound (thin-shell estimate).* The total hidden-sector mode count $N$ receives contributions across a vast frequency range, from $\omega \sim H$ (horizon scale) up to $\omega \sim M_{\text{Pl}}$ (Planck scale). As $H$ changes by $\delta H$ over a Hubble time, only modes in a thin shell near the horizon frequency $\omega \sim H$ actively migrate between visible and hidden sectors; the deep UV modes at $\omega \gg H$ are insensitive to the slowly evolving horizon. The fraction of active modes is of order $\delta N / N \sim H / M_{\text{Pl}}$, since the mode density scales as $\omega^3$ and the shell width is $\sim H$, while the total extends to $M_{\text{Pl}}$. This gives $\nu_{\text{OI}} \sim (H_0/M_{\text{Pl}})^2 \sim 10^{-122}$, which is far too small — it would render the evolution undetectable by any foreseeable experiment.

The physically relevant coefficient therefore lies between these bounds: $10^{-122} \ll \nu_{\text{OI}} \ll 1$. The standard RVM derives $\nu_{\text{RVM}} \sim M_i^2/M_{\text{Pl}}^2 \sim 10^{-3}$ through adiabatic regularization of the vacuum energy, in which the dangerous quartic divergences ($\sim m^4$) cancel and only the mild quadratic ($\sim m^2 H^2$) terms survive [33]. An analogous mechanism may operate here: the $\sqrt{N}$ scaling of §2.1 relates the *ratio* of the two projections to $N$, not the absolute value of $\Lambda_{\text{eff}}$, and the mapping from projection ratio to effective cosmological constant involves the spectral density of the trans-horizon bath, which may introduce a suppression analogous to the $M_i^2/M_{\text{Pl}}^2$ factor. If the heaviest species contributing to the active migration shell has mass $M_*$, one expects $\nu_{\text{OI}} \sim M_*^2/M_{\text{Pl}}^2$ on dimensional grounds, recovering the RVM scaling from a different derivation path. A first-principles calculation from the trans-horizon spectral density is required to determine whether this heuristic is correct; the prediction at this stage is the *functional form* $\Lambda_{\text{eff}} = \Lambda_0 + 3\nu_{\text{OI}}(H^2 - H_0^2)/(8\pi G)$ with $\nu_{\text{OI}} > 0$ and bounded as $10^{-122} < \nu_{\text{OI}} < 1$, but the precise numerical value remains an open problem that constitutes the most important calculational target for future work within this framework.

**Observational status.** Dark energy surveys — DESI, Euclid, and the Vera Rubin Observatory — are measuring the equation of state parameter $w$ with increasing precision. DESI's 2024 Data Release 1 [29] and 2025 Data Release 2 report evidence for evolving dark energy at 2.8σ–4.2σ depending on dataset combination, with the signal strengthening as data has doubled. De Cruz Pérez, Gómez-Valent, and Solà Peracaula [34] tested RVM variants against DESI DR2 + Planck PR4 + supernovae and found preference for dynamical vacuum ($\nu > 0$) at 2.7σ–3.1σ over $\Lambda$CDM, with the best-fit sign consistently indicating energy flow from matter into the vacuum. If confirmed by independent probes, this would be consistent with the framework's prediction that the hidden-sector dimensionality is slowly growing as the cosmological horizon evolves.

**Differentiation from competing models.** The functional form $\Lambda_{\text{eff}} = \Lambda_0 + \beta H^2$ is shared by several independent theoretical programs: the QFT-derived RVM [31], holographic dark energy with Granda-Oliveros cutoff [35], and the stringy RVM from Chern-Simons gravitational anomalies [36]. These models make identical background-cosmology predictions and cannot be distinguished by expansion-history measurements alone. However, the present framework differs from all of them in a structurally important way: it predicts gravitational wave echoes (§4.7) and a stochastic gravitational noise floor from the same hidden-sector physics that sources the cosmological evolution. No purely cosmological RVM or holographic model makes such predictions. The conjunction of confirmed dark energy evolution *and* detected GW echoes would uniquely favor an information-theoretic origin over renormalization-group or holographic derivations.

In the deep future, as $H$ asymptotes to a constant (set by the dark energy density), $N$ saturates at its maximum value and $\Lambda_{\text{eff}}$ stabilizes. An observer in that epoch would inhabit a nearly empty de Sitter void — possessing an extraordinarily precise quantum mechanics (because the trace-out is nearly total and the compression nearly perfect) but describing almost nothing, since essentially the entire universe would have migrated into the hidden sector.

---

## V. CONCLUSION

The incompatibility between quantum mechanics and general relativity is not a bug to be fixed. It is the physical analogue of Gödel incompleteness [5]—the universe demonstrating, through the $10^{122}$ cosmological discrepancy, that observers are inside the system they are trying to describe.

This paper has developed that intuition into a concrete framework. Wolpert’s limits on embedded inference, formalized as complementary projections $\pi_Q$ and $\pi_G$ (§1.3), reinterpret the cosmological constant discrepancy as a direct measurement of $\sim 10^{244}$ hidden degrees of freedom (§2.2). Tracing out this trans-horizon sector via the Nakajima-Zwanzig formalism yields dynamics that are provably non-Markovian for IR horizon modes—the timescale separation required for memoryless evolution collapses when bath, system, and relaxation timescales all equal the Hubble time (§3.2). Conservation-law-enforced correlations and non-perturbative gravitational coupling provide strong physical arguments that the dynamics are further CP-indivisible; through Barandes’ stochastic-quantum correspondence, this recovers the Schrödinger equation as the mandatory description of an embedded observer’s marginal predictions. The framework addresses Bell’s theorem through dynamical indivisibility rather than nonlocality or superdeterminism (§4.2), natively resolves the Wigner’s Friend paradox (§4.3), dissolves the Everettian measure problem (§4.4), reinterprets black hole singularities and the information paradox as consequences of observational incompleteness (§4.5), and explains why string theory’s AdS/CFT correspondence succeeds mathematically while remaining structurally inapplicable to embedded de Sitter observers (§4.6). Because the hidden-sector dimensionality is set by the evolving horizon area, the framework further predicts a slowly time-dependent effective cosmological constant in the Running Vacuum Model form, $\Lambda_{\text{eff}} = \Lambda_0 + 3\nu_{\text{OI}}(H^2 - H_0^2)/(8\pi G)$, derived from information-theoretic rather than renormalization-group arguments and testable by current dark energy surveys (§4.8).

Several open problems remain. The dynamics of the cosmological trace-out are provably non-Markovian, and the physical arguments for full CP-indivisibility—timescale collapse, conservation-law-enforced information backflow, and non-perturbative coupling—are individually strong and mutually reinforcing (§3.2). What remains open is a fully non-perturbative mathematical proof that these conditions jointly force at least one canonical decay rate $\gamma_k(t) < 0$ at some time in the cosmological regime. Additionally, the value of Planck’s constant is recovered as a dimensional consistency condition, but a first-principles derivation from the trans-horizon bath’s spectral density has not yet been achieved. The coefficient $\nu_{\text{OI}}$ in the cosmological evolution prediction requires a first-principles calculation from the trans-horizon spectral density to determine whether the framework’s predicted value matches or differs from the standard RVM coefficient of $\nu \sim 10^{-3}$ (§4.8). On the experimental side, the gravitational wave echo prediction (§4.7) is testable with current and near-future detectors; continued null results would progressively constrain $\epsilon$, while a population of detections across different remnant masses would overconstrain the free parameter entirely.

Recognizing the Schrödinger equation as the macroscopic shadow of $10^{244}$ missing causal variables reframes the unification problem: the goal is not to force quantum mechanics and general relativity into a single formalism, but to understand why an embedded observer necessarily sees them as separate.

---

## DECLARATION OF AI-ASSISTED TECHNOLOGIES
During the preparation of this work, the author used **Claude Opus 4.6 (Anthropic)** and **Gemini 3.1 Pro (Google)** to assist in drafting, refining argumentation, and verifying bibliographic details. The author reviewed and edited the content and takes full responsibility for the publication.

---

## REFERENCES
[1] S. Weinberg, "The cosmological constant problem," *Rev. Mod. Phys.* **61**, 1 (1989).  
[2] J. Martin, "Everything you always wanted to know about the cosmological constant problem (but were afraid to ask)," *C. R. Phys.* **13**, 566–665 (2012).  
[3] S. M. Carroll, "The Cosmological Constant," *Living Rev. Relativ.* **4**, 1 (2001).  
[4] D. H. Wolpert, "Physical limits of inference," *Physica D* **237**, 1257–1281 (2008).  
[5] K. Gödel, "Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I," *Monatsh. Math. Phys.* **38**, 173–198 (1931).  
[6] G. 't Hooft, "Dimensional Reduction in Quantum Gravity," arXiv:gr-qc/9310026 (1993).  
[7] J. Maldacena, "The Large-N Limit of Superconformal Field Theories and Supergravity," *Int. J. Theor. Phys.* **38**, 1113–1133 (1999).  
[8] J. Abedi, H. Dykaar, and N. Afshordi, "Echoes from the Abyss," *Phys. Rev. D* **96**, 082004 (2017).  
[9] A. Arvanitaki and A. A. Geraci, "Detecting High-Frequency Gravitational Waves with Optically Levitated Sensors," *Phys. Rev. Lett.* **110**, 071105 (2013).  
[10] G. W. Gibbons and S. W. Hawking, "Cosmological event horizons, thermodynamics, and particle creation," *Phys. Rev. D* **15**, 2738 (1977).  
[11] J. S. Bell, "On the Einstein Podolsky Rosen paradox," *Physics Physique Fizika* **1**, 195–200 (1964).  
[12] S. Nakajima, "On Quantum Theory of Transport Phenomena," *Prog. Theor. Phys.* **20**, 948–959 (1958).  
[13] R. Zwanzig, "Ensemble Method in the Theory of Irreversibility," *J. Chem. Phys.* **33**, 1338–1341 (1960).  
[14] J. A. Barandes, "The Stochastic-Quantum Theorem," arXiv:2309.03085 (2023).  
[15] J. A. Barandes, "The Stochastic-Quantum Correspondence," *Philosophy of Physics* **3**(1):8 (2025).  
[16] J. A. Barandes, S. Hasan, and J. Kagan, "The CHSH Game, Tsirelson's Bound, and Causal Locality," arXiv:2512.18105 (2025).  
[17] T. Le, F. A. Pollock, T. Paterek, M. Paternostro, and K. Modi, "Divisible quantum dynamics satisfies temporal Tsirelson's bound," *J. Phys. A* **50**, 055302 (2017).  
[18] W. G. Unruh, "Notes on black-hole evaporation," *Phys. Rev. D* **14**, 870 (1976).  
[19] Y. Sekino and L. Susskind, "Fast Scramblers," *JHEP* **2008**, 065 (2008).  
[20] H.-P. Breuer and F. Petruccione, *The Theory of Open Quantum Systems* (Oxford University Press, 2002).  
[21] K. Babu et al., "Unfolding system-environment correlation in open quantum systems: Revisiting master equations and the Born approximation," *Phys. Rev. Research* **6**, 013243 (2024).  
[22] J. Cotler et al., "Black Holes and Random Matrices," *JHEP* **2017**, 118 (2017).  
[23] F. Buscemi, "Complete positivity, Markovianity, and the quantum data-processing inequality, in the presence of initial system-environment correlations," *Phys. Rev. Lett.* **113**, 140502 (2014).
[24] C. Rovelli and F. Vidotto, "Planck Stars," *Class. Quantum Grav.* **31**, 045003 (2014).  
[25] A. Wu, Y. Yan, and L. Ying, "Revisiting Schwarzschild black hole singularity through string theory," *Eur. Phys. J. C* **85**, 168 (2025).  
[26] P. Bueno, P. A. Cano, and R. A. Hennigar, "Regular Black Holes from Pure Gravity," *Phys. Lett. B* **861**, 139260 (2025).  
[27] S. W. Hawking, "Breakdown of predictability in gravitational collapse," *Phys. Rev. D* **14**, 2460 (1976).  
[28] J. Glatthard, "Page curves and typical entanglement in linear optics," *Phys. Rev. D* **109**, L081901 (2024).
[29] DESI Collaboration, "DESI 2024 VI: Cosmological Constraints from the Measurements of Baryon Acoustic Oscillations," arXiv:2404.03002 (2024).  
[30] S. D. H. Hsu, "Entropy bounds and dark energy," *Phys. Lett. B* **594**, 13–16 (2004).  
[31] J. Solà Peracaula, "The cosmological constant problem and running vacuum in the expanding universe," *Phil. Trans. R. Soc. A* **380**, 20210182 (2022).  
[32] J. Solà Peracaula, A. Gómez-Valent, and J. de Cruz Pérez, "Running vacuum in the Universe: phenomenological status in light of the latest observations," *Universe* **9**, 262 (2023).  
[33] C. Moreno-Pulido and J. Solà Peracaula, "Renormalizing the vacuum energy in cosmological spacetime: implications for the cosmological constant problem," *Eur. Phys. J. C* **82**, 551 (2022).  
[34] J. de Cruz Pérez, A. Gómez-Valent, and J. Solà Peracaula, "Dynamical Dark Energy models in light of the latest observations," arXiv:2512.20616 (2025).  
[35] L. N. Granda and A. Oliveros, "Infrared cut-off proposal for the holographic density," *Phys. Lett. B* **669**, 275–277 (2008).  
[36] N. E. Mavromatos, S. Basilakos, and J. Solà Peracaula, "Stringy running vacuum model and current tensions in cosmology," *Class. Quantum Grav.* **41**, 015026 (2024).  
[37] G. Kaplanek and C. P. Burgess, "Hot accelerated qubits: decoherence, thermalization, secular growth and reliable late-time predictions," *JHEP* **2020**, 008 (2020).  
[38] G. Kaplanek and C. P. Burgess, "Qubits on the horizon: decoherence and thermalization near black holes," *JHEP* **2021**, 098 (2021).  
[39] G. Kaplanek and C. P. Burgess, "Hot cosmic qubits: late-time de Sitter evolution and critical slowing down," *JHEP* **2020**, 053 (2020).  
[40] R. C. Bradley, "Basic properties of strong mixing conditions. A survey and some open questions," *Prob. Surveys* **2**, 107–144 (2005).
[41] M. Merkli, "Dynamics of Open Quantum Systems I, Oscillation and Decay," *Quantum* **6**, 615 (2022).
